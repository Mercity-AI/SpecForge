{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAGLE3 Draft Model Evaluation - Interactive\n",
    "\n",
    "This notebook walks through the evaluation process step-by-step so you can inspect:\n",
    "- How prompts are formatted with chat templates\n",
    "- How the loss mask is built\n",
    "- What the draft model predicts vs target model\n",
    "- Per-position accuracy and losses\n",
    "\n",
    "Based on `scripts/eval_eagle.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "from specforge import AutoDraftModelConfig, AutoEagle3DraftModel\n",
    "from specforge.core.eagle3 import OnlineEagle3Model\n",
    "from specforge.data.template import TEMPLATE_REGISTRY\n",
    "from specforge.modeling.target import get_eagle3_target_model\n",
    "from specforge.utils import padding\n",
    "\n",
    "print(\"âœ… Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your paths and parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "config = {\n",
    "    \"draft_checkpoint\": \"./outputs/eagle3-checkpoints/epoch_1_step_12528/\",\n",
    "    \"target_model_path\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"chat_template\": \"qwen\",\n",
    "    \"num_samples\": 3,  # Start with just 3 for interactive exploration\n",
    "    \"ttt_length\": 7,  # Number of positions to predict ahead\n",
    "    \"max_length\": 512,\n",
    "    \"attention_backend\": \"flex_attention\",  # NOT flex_attention_mla for Qwen\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"target_model_backend\": \"hf\",  # Use HuggingFace backend\n",
    "    \"vocab_mapping_path\": None,  # Auto-detect\n",
    "    \"show_first_n_positions\": 10,  # For detailed inspection\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Data\n",
    "\n",
    "These are simple prompts and responses for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROMPTS = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a Python function to calculate factorial.\",\n",
    "]\n",
    "\n",
    "TEST_RESPONSES = [\n",
    "    \"The capital of France is Paris. Paris is located in the north-central part of the country and is the largest city in France.\",\n",
    "    \"Quantum computing uses quantum bits or qubits that can exist in multiple states simultaneously, unlike classical bits that are either 0 or 1.\",\n",
    "    \"Here's a Python function to calculate factorial:\\n\\ndef factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)\",\n",
    "]\n",
    "\n",
    "num_samples = min(config[\"num_samples\"], len(TEST_PROMPTS))\n",
    "print(f\"Using {num_samples} test samples\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"\\n{i+1}. Prompt: {TEST_PROMPTS[i]}\")\n",
    "    print(f\"   Response: {TEST_RESPONSES[i][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer and Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"target_model_path\"])\n",
    "\n",
    "print(\"\\nTokenizer info:\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  BOS token: {tokenizer.bos_token} (id={tokenizer.bos_token_id})\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (id={tokenizer.eos_token_id})\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Load chat template\n",
    "template = TEMPLATE_REGISTRY.get(config[\"chat_template\"])\n",
    "print(f\"\\nChat template ({config['chat_template']}):\")\n",
    "print(f\"  User header: {repr(template.user_header)}\")\n",
    "print(f\"  Assistant header: {repr(template.assistant_header)}\")\n",
    "print(f\"  End of turn: {repr(template.end_of_turn_token)}\")\n",
    "print(f\"  System prompt: {repr(template.system_prompt[:80] if template.system_prompt else None)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format a Single Sample (Inspect in Detail)\n",
    "\n",
    "Let's look at how the first sample is formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first sample\n",
    "sample_idx = 0\n",
    "prompt = TEST_PROMPTS[sample_idx]\n",
    "response = TEST_RESPONSES[sample_idx]\n",
    "\n",
    "print(f\"Sample {sample_idx + 1}:\")\n",
    "print(f\"  Prompt: {prompt}\")\n",
    "print(f\"  Response: {response}\")\n",
    "\n",
    "# Build messages\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "if template.system_prompt:\n",
    "    messages = [{\"role\": \"system\", \"content\": template.system_prompt}] + messages\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "print(f\"\\nMessages structure ({len(messages)} messages):\")\n",
    "for i, msg in enumerate(messages):\n",
    "    content_preview = msg['content'][:60] + \"...\" if len(msg['content']) > 60 else msg['content']\n",
    "    print(f\"  {i+1}. {msg['role']}: {content_preview}\")\n",
    "\n",
    "# Apply chat template\n",
    "try:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    print(\"\\nâœ… Chat template applied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error applying chat template: {e}\")\n",
    "    # Fallback\n",
    "    text = f\"{template.user_header}{prompt}{template.end_of_turn_token}{template.assistant_header}{response}{template.end_of_turn_token}\"\n",
    "    print(\"   Using fallback template\")\n",
    "\n",
    "print(f\"\\nFormatted text ({len(text)} chars):\")\n",
    "print(\"=\" * 80)\n",
    "print(text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tokenize and Build Loss Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "inputs = tokenizer(\n",
    "    text, \n",
    "    return_tensors=\"pt\", \n",
    "    return_offsets_mapping=True,\n",
    "    max_length=config[\"max_length\"], \n",
    "    truncation=True,\n",
    "    add_special_tokens=False,  # Match training behavior\n",
    ")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "offsets = inputs[\"offset_mapping\"][0]\n",
    "\n",
    "print(f\"Tokenization:\")\n",
    "print(f\"  Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"  Total tokens: {input_ids.shape[1]}\")\n",
    "\n",
    "# Show first 20 tokens\n",
    "print(f\"\\nFirst 20 tokens:\")\n",
    "for i in range(min(20, input_ids.shape[1])):\n",
    "    token_id = input_ids[0, i].item()\n",
    "    token_text = tokenizer.decode([token_id], skip_special_tokens=False)\n",
    "    print(f\"  {i:3d}: {token_id:6d} -> {repr(token_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loss mask (only assistant tokens count)\n",
    "loss_mask = torch.zeros(input_ids.shape[1], dtype=torch.long)\n",
    "\n",
    "user_sep = f\"{template.end_of_turn_token or ''}{template.user_header}\"\n",
    "asst_sep = f\"{template.end_of_turn_token or ''}{template.assistant_header}\"\n",
    "assistant_pattern = re.escape(asst_sep) + r\"(.*?)(?=\" + re.escape(user_sep) + \"|$)\"\n",
    "\n",
    "print(f\"Loss mask pattern:\")\n",
    "print(f\"  User separator: {repr(user_sep)}\")\n",
    "print(f\"  Assistant separator: {repr(asst_sep)}\")\n",
    "print(f\"  Regex pattern: {assistant_pattern[:100]}...\")\n",
    "\n",
    "matches_found = 0\n",
    "for match in re.finditer(assistant_pattern, text, re.DOTALL):\n",
    "    matches_found += 1\n",
    "    start_char = match.start(1)\n",
    "    end_char = match.end(1)\n",
    "    \n",
    "    matched_text = text[start_char:end_char]\n",
    "    print(f\"\\n  Match {matches_found}: chars [{start_char}:{end_char}]\")\n",
    "    print(f\"    Content preview: {repr(matched_text[:80])}...\")\n",
    "    \n",
    "    for idx, (tok_start, tok_end) in enumerate(offsets):\n",
    "        if tok_end > start_char and tok_start <= end_char:\n",
    "            loss_mask[idx] = 1\n",
    "\n",
    "if matches_found == 0:\n",
    "    print(\"\\nâš ï¸  WARNING: No assistant spans found! Loss mask will be all zeros.\")\n",
    "else:\n",
    "    assistant_tokens = loss_mask.sum().item()\n",
    "    total_tokens = input_ids.shape[1]\n",
    "    print(f\"\\nâœ… Loss mask built: {assistant_tokens}/{total_tokens} tokens are assistant responses ({assistant_tokens/total_tokens:.1%})\")\n",
    "\n",
    "loss_mask = loss_mask.unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss mask\n",
    "print(\"Token-by-token breakdown (first 30 tokens):\")\n",
    "print(f\"{'Idx':<5} {'TokenID':<8} {'Token':<25} {'Mask':<5} {'Role'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(min(30, input_ids.shape[1])):\n",
    "    token_id = input_ids[0, i].item()\n",
    "    token_text = tokenizer.decode([token_id], skip_special_tokens=False)\n",
    "    mask_val = loss_mask[0, i].item()\n",
    "    role = \"ASST\" if mask_val == 1 else \"USER/SYS\"\n",
    "    \n",
    "    # Truncate long tokens\n",
    "    token_display = repr(token_text)[:23]\n",
    "    print(f\"{i:<5} {token_id:<8} {token_display:<25} {mask_val:<5} {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Models\n",
    "\n",
    "Now let's load the draft and target models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.abspath(config[\"draft_checkpoint\"])\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "print(f\"Loading draft model from: {checkpoint_path}\")\n",
    "draft_model = AutoEagle3DraftModel.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    attention_backend=config[\"attention_backend\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(config[\"device\"]).eval()\n",
    "\n",
    "draft_params = sum(p.numel() for p in draft_model.parameters())\n",
    "print(f\"   Parameters: {draft_params/1e6:.1f}M\")\n",
    "print(f\"   Config: {draft_model.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab mapping\n",
    "vocab_mapping_path = config[\"vocab_mapping_path\"]\n",
    "if vocab_mapping_path is None:\n",
    "    if os.path.exists(os.path.join(checkpoint_path, \"vocab_mapping.pt\")):\n",
    "        vocab_mapping_path = os.path.join(checkpoint_path, \"vocab_mapping.pt\")\n",
    "    else:\n",
    "        # Try common cache locations\n",
    "        for cache_dir in [\"./cache/vocab_mapping\", \"../cache/vocab_mapping\"]:\n",
    "            if os.path.isdir(cache_dir):\n",
    "                for f in os.listdir(cache_dir):\n",
    "                    if f.endswith(\".pt\"):\n",
    "                        vocab_mapping_path = os.path.join(cache_dir, f)\n",
    "                        break\n",
    "            if vocab_mapping_path:\n",
    "                break\n",
    "\n",
    "if vocab_mapping_path and os.path.exists(vocab_mapping_path):\n",
    "    draft_model.load_vocab_mapping(vocab_mapping_path)\n",
    "    print(f\"âœ… Loaded vocab mapping from: {vocab_mapping_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No vocab mapping found! Results may be inaccurate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLoading target model: {config['target_model_path']}\")\n",
    "target_model = get_eagle3_target_model(\n",
    "    pretrained_model_name_or_path=config[\"target_model_path\"],\n",
    "    backend=config[\"target_model_backend\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=config[\"device\"],\n",
    ")\n",
    "\n",
    "# Set aux hidden states layers\n",
    "draft_config = draft_model.config\n",
    "if (\n",
    "    hasattr(draft_config, \"eagle_config\")\n",
    "    and draft_config.eagle_config\n",
    "    and \"eagle_aux_hidden_state_layer_ids\" in draft_config.eagle_config\n",
    "):\n",
    "    aux_layers = draft_config.eagle_config[\"eagle_aux_hidden_state_layer_ids\"]\n",
    "    target_model.set_aux_hidden_states_layers(aux_layers)\n",
    "    print(f\"   Aux hidden state layers: {aux_layers}\")\n",
    "else:\n",
    "    target_model.set_aux_hidden_states_layers()\n",
    "    print(\"   Using default aux hidden state layers\")\n",
    "\n",
    "# Estimate target params\n",
    "target_config = AutoConfig.from_pretrained(config[\"target_model_path\"])\n",
    "target_params = getattr(target_config, \"num_parameters\", None)\n",
    "if target_params is None:\n",
    "    h = target_config.hidden_size\n",
    "    n_layers = target_config.num_hidden_layers\n",
    "    vocab = target_config.vocab_size\n",
    "    inter = getattr(target_config, \"intermediate_size\", h * 4)\n",
    "    target_params = vocab * h * 2 + n_layers * (h * h * 4 + h * inter * 3)\n",
    "print(f\"   Parameters: ~{target_params/1e9:.1f}B (estimated)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build EAGLE3 model\n",
    "eagle3_model = OnlineEagle3Model(\n",
    "    draft_model=draft_model,\n",
    "    length=config[\"ttt_length\"],\n",
    "    attention_backend=config[\"attention_backend\"],\n",
    ").to(config[\"device\"]).eval()\n",
    "\n",
    "print(f\"âœ… EAGLE3 model ready (TTT length={config['ttt_length']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Evaluation on Sample\n",
    "\n",
    "Let's evaluate on our first sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move tensors to device\n",
    "input_ids = input_ids.to(config[\"device\"])\n",
    "attention_mask = attention_mask.to(config[\"device\"])\n",
    "loss_mask = loss_mask.to(config[\"device\"])\n",
    "\n",
    "print(f\"Running evaluation on sample...\")\n",
    "print(f\"  Input shape: {input_ids.shape}\")\n",
    "print(f\"  Device: {config['device']}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get target model data\n",
    "    eagle3_data = target_model.generate_eagle3_data(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        loss_mask=loss_mask,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEAGLE3 data generated:\")\n",
    "    print(f\"  Input IDs shape: {eagle3_data.input_ids.shape}\")\n",
    "    print(f\"  Hidden states shape: {eagle3_data.hidden_states.shape}\")\n",
    "    print(f\"  Target shape: {eagle3_data.target.shape}\")\n",
    "    print(f\"  Loss mask shape: {eagle3_data.loss_mask.shape}\")\n",
    "    \n",
    "    # Run EAGLE3 forward pass\n",
    "    plosses, _, acces = eagle3_model(\n",
    "        input_ids=eagle3_data.input_ids,\n",
    "        attention_mask=eagle3_data.attention_mask,\n",
    "        loss_mask=eagle3_data.loss_mask,\n",
    "        target=eagle3_data.target,\n",
    "        hidden_states=eagle3_data.hidden_states,\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete in {elapsed:.2f}s\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Number of TTT positions: {len(plosses)}\")\n",
    "for i, (loss, acc) in enumerate(zip(plosses, acces)):\n",
    "    print(f\"  Position {i}: Loss={loss.item():.4f}, Accuracy={acc.item():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Prediction Analysis\n",
    "\n",
    "Let's see what the draft model predicted vs what the target model said:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_draft_predictions(eagle3_model, eagle3_data, ttt_length, attention_backend):\n",
    "    \"\"\"Get draft model predictions for each TTT position.\"\"\"\n",
    "    draft_model = eagle3_model.draft_model\n",
    "    input_ids = eagle3_data.input_ids\n",
    "    attention_mask = eagle3_data.attention_mask\n",
    "    hidden_states = eagle3_data.hidden_states\n",
    "    \n",
    "    batch_size, seq_length = input_ids.shape\n",
    "    device = hidden_states.device\n",
    "    \n",
    "    # Project hidden states\n",
    "    hidden_states = draft_model.project_hidden_states(hidden_states)\n",
    "    \n",
    "    # Initialize cache\n",
    "    if attention_backend == \"sdpa\" or attention_backend == \"flex_attention_mla\":\n",
    "        cache_hidden = [[], []]\n",
    "        past_key_values = None\n",
    "    elif attention_backend == \"flex_attention\":\n",
    "        cache_hidden = None\n",
    "        past_key_values = DynamicCache()\n",
    "    \n",
    "    # Store predictions for each TTT position\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Initialize position_ids\n",
    "    past_key_values_length = 0\n",
    "    position_ids = torch.arange(\n",
    "        past_key_values_length,\n",
    "        seq_length + past_key_values_length,\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "    position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "    \n",
    "    for idx in range(ttt_length):\n",
    "        # Embed input ids\n",
    "        inputs_embeds = draft_model.embed_input_ids(input_ids)\n",
    "        inputs_embeds = inputs_embeds.to(hidden_states.dtype)\n",
    "        \n",
    "        # Run backbone\n",
    "        hidden_states_out = draft_model.backbone(\n",
    "            input_embeds=inputs_embeds,\n",
    "            hidden_states=hidden_states,\n",
    "            cache_hidden=cache_hidden,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        \n",
    "        hidden_states = hidden_states_out\n",
    "        \n",
    "        # Get logits and predictions\n",
    "        logits = draft_model.compute_logits(hidden_states)\n",
    "        predictions = logits.argmax(dim=-1)  # [batch, seq_len]\n",
    "        all_predictions.append(predictions)\n",
    "        \n",
    "        # Pad for next iteration (except last)\n",
    "        if idx < ttt_length - 1:\n",
    "            input_ids = padding(input_ids, left=False)\n",
    "            position_ids = padding(position_ids, left=False)\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# Get draft predictions\n",
    "print(\"Getting draft predictions...\")\n",
    "draft_predictions = get_draft_predictions(\n",
    "    eagle3_model, \n",
    "    eagle3_data, \n",
    "    config[\"ttt_length\"],\n",
    "    config[\"attention_backend\"]\n",
    ")\n",
    "print(f\"âœ… Got predictions for {len(draft_predictions)} TTT positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "target_tokens = eagle3_data.target.argmax(dim=-1)[0]  # [seq_len]\n",
    "input_tokens = eagle3_data.input_ids[0].tolist()  # [seq_len]\n",
    "\n",
    "num_positions_to_show = min(config[\"show_first_n_positions\"], len(input_tokens) - config[\"ttt_length\"])\n",
    "\n",
    "print(f\"\\nðŸ“Š Detailed Predictions (first {num_positions_to_show} positions)\")\n",
    "print(f\"\\nLegend: TTT=Time-to-Target offset\")\n",
    "print(f\"  TTT 0 = predicting next token (position i predicts i+0)\")\n",
    "print(f\"  TTT 1 = predicting one ahead (position i predicts i+1)\")\n",
    "print(f\"  etc.\\n\")\n",
    "\n",
    "print(f\"{'Pos':<4} {'TTT':<4} {'Context (last 2 tokens)':<35} {'Draft Token':<20} {'Target Token':<20} {'Match'}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for pos_idx in range(num_positions_to_show):\n",
    "    # Show context\n",
    "    context_start = max(0, pos_idx - 1)\n",
    "    context_tokens = input_tokens[context_start:pos_idx+1]\n",
    "    context_text = tokenizer.decode(context_tokens, skip_special_tokens=False)\n",
    "    context_text = repr(context_text)[1:-1]  # Escape\n",
    "    context_text = (\"...\" + context_text[-32:]) if len(context_text) > 35 else context_text\n",
    "    \n",
    "    # For each TTT position\n",
    "    for ttt_idx in range(config[\"ttt_length\"]):\n",
    "        target_pos = pos_idx + ttt_idx\n",
    "        \n",
    "        if target_pos >= len(target_tokens):\n",
    "            break\n",
    "        \n",
    "        # Get predictions\n",
    "        draft_pred = draft_predictions[ttt_idx][0, pos_idx].item()\n",
    "        target_token = target_tokens[target_pos].item()\n",
    "        \n",
    "        match = \"âœ“\" if draft_pred == target_token else \"âœ—\"\n",
    "        \n",
    "        # Decode\n",
    "        draft_text = tokenizer.decode([draft_pred], skip_special_tokens=False)\n",
    "        target_text = tokenizer.decode([target_token], skip_special_tokens=False)\n",
    "        \n",
    "        draft_text = repr(draft_text)[1:-1][:18]\n",
    "        target_text = repr(target_text)[1:-1][:18]\n",
    "        \n",
    "        # Only show context for first TTT position\n",
    "        if ttt_idx == 0:\n",
    "            print(f\"{pos_idx:<4} {ttt_idx:<4} {context_text:<35} {draft_text:<20} {target_text:<20} {match}\")\n",
    "        else:\n",
    "            print(f\"{'':4} {ttt_idx:<4} {'':35} {draft_text:<20} {target_text:<20} {match}\")\n",
    "    \n",
    "    # Separator every 3 positions\n",
    "    if pos_idx < num_positions_to_show - 1 and pos_idx % 3 == 2:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mean_acc = sum(acc.item() for acc in acces) / len(acces)\n",
    "mean_loss = sum(loss.item() for loss in plosses) / len(plosses)\n",
    "\n",
    "print(f\"\\nPer-Position Results:\")\n",
    "for i, (acc, loss) in enumerate(zip(acces, plosses)):\n",
    "    bar = \"â–ˆ\" * int(acc.item() * 30) + \"â–‘\" * (30 - int(acc.item() * 30))\n",
    "    print(f\"  TTT Position {i}: {bar} {acc.item():.1%}  (loss: {loss.item():.4f})\")\n",
    "\n",
    "print(f\"\\nOverall:\")\n",
    "print(f\"  Mean Accuracy: {mean_acc:.1%}\")\n",
    "print(f\"  Mean Loss:     {mean_loss:.4f}\")\n",
    "print(f\"  Time:          {elapsed:.2f}s\")\n",
    "print(f\"  Tokens:        {input_ids.shape[1]}\")\n",
    "print(f\"  Throughput:    {input_ids.shape[1] / elapsed:.1f} tokens/s\")\n",
    "\n",
    "# Quality interpretation\n",
    "print(f\"\\nQuality Assessment:\")\n",
    "if mean_acc >= 0.7:\n",
    "    print(\"  âœ… EXCELLENT - Expected speedup: 2.5x - 3.5x\")\n",
    "elif mean_acc >= 0.5:\n",
    "    print(\"  âš ï¸  GOOD - Expected speedup: 1.5x - 2.5x\")\n",
    "elif mean_acc >= 0.3:\n",
    "    print(\"  âš ï¸  FAIR - Expected speedup: 1.2x - 1.5x\")\n",
    "else:\n",
    "    print(\"  âŒ NEEDS WORK - Train longer or tune hyperparameters\")\n",
    "\n",
    "# Accuracy decay\n",
    "if len(acces) > 1:\n",
    "    decay = (acces[0].item() - acces[-1].item()) / acces[0].item() * 100\n",
    "    print(f\"\\n  Accuracy decay (TTT 0â†’{len(acces)-1}): {decay:.1f}%\")\n",
    "    if decay > 30:\n",
    "        print(f\"    âš ï¸  High decay - model struggles with longer predictions\")\n",
    "    else:\n",
    "        print(f\"    âœ… Low decay - consistent predictions across TTT positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run on Multiple Samples (Optional)\n",
    "\n",
    "Now let's run on all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "all_accs = [[] for _ in range(config[\"ttt_length\"])]\n",
    "all_losses = [[] for _ in range(config[\"ttt_length\"])]\n",
    "total_tokens = 0\n",
    "total_time = 0.0\n",
    "\n",
    "for i in tqdm(range(num_samples), desc=\"Evaluating samples\"):\n",
    "    prompt = TEST_PROMPTS[i]\n",
    "    response = TEST_RESPONSES[i]\n",
    "    \n",
    "    # Build conversation\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    if template.system_prompt:\n",
    "        messages = [{\"role\": \"system\", \"content\": template.system_prompt}] + messages\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    # Tokenize\n",
    "    try:\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    except Exception:\n",
    "        text = f\"{template.user_header}{prompt}{template.end_of_turn_token}{template.assistant_header}{response}{template.end_of_turn_token}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        return_offsets_mapping=True,\n",
    "        max_length=config[\"max_length\"], \n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(config[\"device\"])\n",
    "    attention_mask = inputs[\"attention_mask\"].to(config[\"device\"])\n",
    "    offsets = inputs[\"offset_mapping\"][0]\n",
    "    \n",
    "    # Build loss mask\n",
    "    loss_mask = torch.zeros(input_ids.shape[1], dtype=torch.long, device=config[\"device\"])\n",
    "    for match in re.finditer(assistant_pattern, text, re.DOTALL):\n",
    "        start_char = match.start(1)\n",
    "        end_char = match.end(1)\n",
    "        for idx, (tok_start, tok_end) in enumerate(offsets):\n",
    "            if tok_end > start_char and tok_start <= end_char:\n",
    "                loss_mask[idx] = 1\n",
    "    loss_mask = loss_mask.unsqueeze(0)\n",
    "    \n",
    "    # Evaluate\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        eagle3_data = target_model.generate_eagle3_data(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            loss_mask=loss_mask,\n",
    "        )\n",
    "        \n",
    "        plosses, _, acces = eagle3_model(\n",
    "            input_ids=eagle3_data.input_ids,\n",
    "            attention_mask=eagle3_data.attention_mask,\n",
    "            loss_mask=eagle3_data.loss_mask,\n",
    "            target=eagle3_data.target,\n",
    "            hidden_states=eagle3_data.hidden_states,\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    total_time += elapsed\n",
    "    total_tokens += input_ids.shape[1]\n",
    "    \n",
    "    # Collect metrics\n",
    "    for j, (acc, loss) in enumerate(zip(acces, plosses)):\n",
    "        all_accs[j].append(acc.item())\n",
    "        all_losses[j].append(loss.item())\n",
    "    \n",
    "    mean_acc = sum(a for a in acces) / len(acces)\n",
    "    print(f\"  Sample {i+1}: acc={mean_acc.item():.1%} ({elapsed:.2f}s)\")\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results\n",
    "per_pos_acc = [sum(accs) / len(accs) for accs in all_accs]\n",
    "per_pos_loss = [sum(losses) / len(losses) for losses in all_losses]\n",
    "mean_acc = sum(per_pos_acc) / len(per_pos_acc)\n",
    "mean_loss = sum(per_pos_loss) / len(per_pos_loss)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"FINAL RESULTS ({num_samples} samples)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nPer-Position Accuracy:\")\n",
    "for i, (acc, loss) in enumerate(zip(per_pos_acc, per_pos_loss)):\n",
    "    bar = \"â–ˆ\" * int(acc * 40) + \"â–‘\" * (40 - int(acc * 40))\n",
    "    print(f\"  Position {i}: {bar} {acc:.1%}  (loss: {loss:.4f})\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Mean Accuracy:    {mean_acc:.1%}\")\n",
    "print(f\"  Mean Loss:        {mean_loss:.4f}\")\n",
    "print(f\"  Total Time:       {total_time:.2f}s\")\n",
    "print(f\"  Tokens Processed: {total_tokens:,}\")\n",
    "print(f\"  Throughput:       {total_tokens / total_time:.1f} tokens/s\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(range(len(per_pos_acc)), per_pos_acc, marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('TTT Position', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Draft Model Accuracy by TTT Position', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.axhline(y=mean_acc, color='r', linestyle='--', label=f'Mean: {mean_acc:.1%}')\n",
    "ax1.legend()\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(range(len(per_pos_loss)), per_pos_loss, marker='s', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel('TTT Position', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.set_title('Draft Model Loss by TTT Position', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=mean_loss, color='r', linestyle='--', label=f'Mean: {mean_loss:.4f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the evaluation process:\n",
    "\n",
    "1. **Modify the test samples** to use your own prompts/responses\n",
    "2. **Try different chat templates** if using a different model\n",
    "3. **Adjust TTT length** to see how performance changes\n",
    "4. **Run on more samples** for more robust statistics\n",
    "5. **Compare different checkpoints** from your training run\n",
    "\n",
    "Key insights:\n",
    "- **Loss mask** determines which tokens contribute to loss (only assistant responses)\n",
    "- **TTT predictions** show how far ahead the draft model can predict accurately\n",
    "- **Accuracy decay** indicates if the model struggles with longer-term predictions\n",
    "- **High accuracy (>60%)** suggests good speculative decoding speedup potential"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
